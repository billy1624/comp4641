{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 601,
     "status": "ok",
     "timestamp": 1587199912333,
     "user": {
      "displayName": "Alan Ho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgtT_W3WxZjHz1rKR5CS1dYx6Pu-NRtQp1y7FPosQ=s64",
      "userId": "08031624836839329740"
     },
     "user_tz": -480
    },
    "id": "7Ry9171Gdtff",
    "outputId": "2c00056f-ca72-4f91-a466-2a05a41e77d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from torchtext import *\n",
    "from torchtext.data import *\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotions_list = ['Joy', 'Trust', 'Fear', 'Surprise', 'Sadness', 'Disgust', 'Anger', 'Anticipation', 'Neutral']\n",
    "emotions_list = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R65AvIDcddO_"
   },
   "outputs": [],
   "source": [
    "# 5.3.1\n",
    "txt_field = data.Field(tokenize=word_tokenize, lower=True, include_lengths=True, batch_first=True)\n",
    "label_field = data.Field(sequential=False, use_vocab=False, batch_first=True)\n",
    "\n",
    "# dataset_fields = [('text', txt_field), ('Joy', label_field), \n",
    "#                 ('Trust', label_field), ('Fear', label_field), ('Surprise', label_field), \n",
    "#                 ('Sadness', label_field), ('Disgust', label_field), ('Anger', label_field), \n",
    "#                 ('Anticipation', label_field), ('Neutral', label_field)]\n",
    "\n",
    "dataset_fields = [('text', txt_field), ('anger', label_field), \n",
    "                ('anticipation', label_field), ('disgust', label_field), ('fear', label_field), \n",
    "                ('joy', label_field), ('sadness', label_field), ('surprise', label_field), \n",
    "                ('trust', label_field), ('neutral', label_field)]\n",
    "\n",
    "\n",
    "# train, test= TabularDataset.splits(path='./', train='train.csv', test='valid.csv', format='csv', \n",
    "#     fields = dataset_fields, skip_header=True)\n",
    "\n",
    "train, test= TabularDataset.splits(path='./', train='train-80000.csv', test='valid-80000.csv', format='csv', \n",
    "    fields = dataset_fields, skip_header=True)\n",
    "\n",
    "\n",
    "label_field.build_vocab(train)\n",
    "txt_field.build_vocab(train,vectors=vocab.Vectors(\"glove.840B.300d.txt\"),max_size=20000)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=32, sort_key=lambda x: len(x.text), sort_within_batch=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_iter:\n",
    "#     print(batch.__dict__.keys())\n",
    "#     print(batch.fear)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x_var, y_vars):\n",
    "        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars # 传入自变量x列表和因变量y列表\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            x = getattr(batch, self.x_var) # 在这个封装中只有一个自变量\n",
    "\n",
    "            if self.y_vars is not None: # 把所有因变量cat成一个向量\n",
    "                temp = [getattr(batch, feat).unsqueeze(1) for feat in self.y_vars]\n",
    "                y = torch.cat(temp, dim=1).float()\n",
    "            else:\n",
    "                y = torch.zeros((1))\n",
    "\n",
    "            yield (x, y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "train_dl = BatchWrapper(train_iter, \"text\", emotions_list)\n",
    "test_dl = BatchWrapper(test_iter, \"text\", emotions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((tensor([[    5,    19,    29,   679,    68,    19],\n",
      "        [  177,  3049,   141,   763,    11,  3996],\n",
      "        [  108,   158,    20,  2059,     3,   433],\n",
      "        [  196,   322,    28,   927,    59,     9],\n",
      "        [   28,  3535,  3463,    25,   138,     9],\n",
      "        [  440,     5,    19,   785,  1573,     9],\n",
      "        [    3,  1884,   336,  4247,   677,     2],\n",
      "        [ 1271,     2,  1084,     2,   130,     2],\n",
      "        [    4,   158,    24,     3,  4591,     2],\n",
      "        [  103,     6,   416,     3,   357,    15],\n",
      "        [  361,    33,     6,     8,   880,     2],\n",
      "        [15597,     9,  1000,     9,   774,    63],\n",
      "        [ 1634,   731,     5,   661,   175,   331],\n",
      "        [   33,   241,   140,    83,    89,     5],\n",
      "        [ 1357,    25,  2568,     4,  1430,     2],\n",
      "        [   54,    12,   290,     9,  1073,    19],\n",
      "        [    5,   649,    22,   262,     6,  1884],\n",
      "        [  125,   653,    50,    94,    20,   135],\n",
      "        [  205,   903,   307,  1186,   213,     5],\n",
      "        [   81,    97,  4625,    28,    14,  1378],\n",
      "        [   13,  7428,    32,    54,    14,     9],\n",
      "        [  391,   205,     5,    15,    19,    73],\n",
      "        [ 2428,    83,   442,    11,   525,    19],\n",
      "        [  352,   844,   731,   141,    28,     5],\n",
      "        [   55,    24,   331,    55,    24,   331],\n",
      "        [   24,    93,     9,    24,   477,     9],\n",
      "        [   33,   241,   140,    83,    89,     5],\n",
      "        [  245,    55,   217,    14,   101,     0],\n",
      "        [   46,     2,   193,     2,   209,     2],\n",
      "        [  429,    88,  5096, 10052, 12254, 18802],\n",
      "        [   46,     3,   940,     6,    29,     2],\n",
      "        [  646,  9222,   121,    10,   251,     1]], device='cuda:0'), tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 5], device='cuda:0')), tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 1., 1., 1., 0., 1., 1., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 1., 1., 1., 0., 1., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 1., 0., 0., 0.]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dl:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20002"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = len(txt_field.vocab)\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "output_dim = 9\n",
    "dropout = 0.5\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_len):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (last_hidden_state, last_cell_state) = self.rnn(embedded)\n",
    "        linear_input = last_hidden_state[-1]\n",
    "        return self.fc(self.dropout(linear_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rEPioC5lh62N"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    temp = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model architecture:\\n\\n', model)\n",
    "    print(f'\\nThe model has {temp:,} trainable parameters')\n",
    "\n",
    "model = LSTM(input_dim, embedding_dim, hidden_dim, output_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1587201163361,
     "user": {
      "displayName": "Alan Ho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgtT_W3WxZjHz1rKR5CS1dYx6Pu-NRtQp1y7FPosQ=s64",
      "userId": "08031624836839329740"
     },
     "user_tz": -480
    },
    "id": "bLwuLr0Bh7Gn",
    "outputId": "3daee6e4-bdc2-4bba-bae2-3fc2257e8354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model architecture:\n",
      "\n",
      " LSTM(\n",
      "  (embedding): Embedding(20002, 64)\n",
      "  (rnn): LSTM(64, 128, batch_first=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=9, bias=True)\n",
      ")\n",
      "\n",
      "The model has 1,380,617 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QwGYSUoVh8Tz"
   },
   "outputs": [],
   "source": [
    "def test_accuracy(tensor, y):\n",
    "    preds = torch.round(torch.sigmoid(tensor))\n",
    "    correct = (preds == y).float()\n",
    "    return correct.sum() / len(correct) * 100\n",
    "\n",
    "\n",
    "def save_checkpoint(save_path, model, optimizer, val_loss):\n",
    "    if save_path==None:\n",
    "        return\n",
    "    save_path = save_path \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'val_loss': val_loss}\n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, save_path):\n",
    "    state_dict = torch.load(save_path)\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    val_loss = state_dict['val_loss']\n",
    "    print(f'Model loaded from <== {save_path}')\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def TRAIN(net, train_iter, valid_iter, num_epochs, eval_every, total_step, criterion, optimizer, val_loss, device, save_name):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    running_num = 0\n",
    "    global_step = 0\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    count = []\n",
    "    if val_loss==None:\n",
    "        best_val_loss = float(\"Inf\")  \n",
    "    else: \n",
    "        best_val_loss=val_loss\n",
    "\n",
    "    since = time.time()\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        \n",
    "        for batch in train_iter: ## batch is ((text, text_len_array), 2d_labels), text_len_array hv all same value thx to BucketIterator\n",
    "            \n",
    "            net.train()\n",
    "\n",
    "            '''Training of the model'''\n",
    "            # Forward pass\n",
    "            text = batch[0][0] \n",
    "            text_len = batch[0][1][0].item()\n",
    "            label = batch[1]\n",
    "            \n",
    "            outputs = net(text, text_len).squeeze(1)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            '''Evaluating the model every x steps'''\n",
    "            if global_step % eval_every == 0:\n",
    "                with torch.no_grad():\n",
    "                    net.eval()\n",
    "                    val_running_loss = 0.0\n",
    "                    for val_batch in valid_iter:\n",
    "                        val_text = val_batch[0][0]\n",
    "                        val_text_len = val_batch[0][1][0].item()\n",
    "                        val_label = val_batch[1]\n",
    "                        \n",
    "                        val_outputs = net(val_text, val_text_len).squeeze(1)\n",
    "                        val_loss = criterion(val_outputs, val_label)\n",
    "                        val_running_loss += val_loss.item()\n",
    "\n",
    "                    average_train_loss = running_loss / eval_every\n",
    "                    average_val_loss = val_running_loss / len(valid_iter)\n",
    "                    train_loss.append(average_train_loss)\n",
    "                    valid_loss.append(average_val_loss)\n",
    "                    count.append(global_step)\n",
    "\n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                          .format(epoch+1, num_epochs, global_step, total_step, average_train_loss, average_val_loss))\n",
    "\n",
    "                    running_loss = 0.0\n",
    "                    \n",
    "                    if average_val_loss < best_val_loss:\n",
    "                        best_val_loss = average_val_loss\n",
    "                        save_checkpoint(save_name, net, optimizer, best_val_loss)\n",
    "\n",
    "    print('Finished Training, training time: %.4f' % (time.time() - since))\n",
    "\n",
    "    print('training loss: %.4f\\nvalidation loss: %.4f' % (train_loss[-1], valid_loss[-1]))\n",
    "    plot_x = count\n",
    "    p1, = plt.plot(plot_x, train_loss, color='red', linewidth=1, label='training loss')\n",
    "    p2, = plt.plot(plot_x, valid_loss,  color='blue', linewidth=1, label='validation loss')\n",
    "    plt.legend(handles=[p1, p2], loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('number of evaluation')\n",
    "    plt.title('loss over number of evaluation')\n",
    "    plt.show()\n",
    "\n",
    "    return train_loss, valid_loss\n",
    "\n",
    "\n",
    "def eval(model, iterator):\n",
    "    \n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch[0][0]\n",
    "            text_len = batch[0][1][0].item()\n",
    "            label = batch[1]\n",
    "\n",
    "            outputs = model(text, text_len).squeeze(1)\n",
    "            acc = test_accuracy(outputs, label) / 9\n",
    "\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    avg_acc = epoch_acc / len(iterator)\n",
    "    print(f'Test accuracy: {avg_acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 858
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 260463,
     "status": "ok",
     "timestamp": 1587202476222,
     "user": {
      "displayName": "Alan Ho",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgtT_W3WxZjHz1rKR5CS1dYx6Pu-NRtQp1y7FPosQ=s64",
      "userId": "08031624836839329740"
     },
     "user_tz": -480
    },
    "id": "80n-00m-moqO",
    "outputId": "6768d3cc-d8ef-4d47-9e26-32e6b3995fc2"
   },
   "outputs": [],
   "source": [
    "model = LSTM(input_dim, embedding_dim, hidden_dim, output_dim, dropout).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "num_epochs = 15\n",
    "eval_every = 1000\n",
    "total_step = len(train_iter)*num_epochs\n",
    "best_val_loss = None\n",
    "save_path = f'tweet_net.pt'\n",
    "\n",
    "# train_loss, valid_loss = TRAIN(model, train_dl, test_dl, num_epochs, eval_every, total_step, criterion, optimizer, \n",
    "#       best_val_loss, device, save_path)\n",
    "\n",
    "#eval(model2, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== tweet_net.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18419405172020198"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_checkpoint(model, optimizer, f'tweet_net.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 94.16180645751953%\n"
     ]
    }
   ],
   "source": [
    "eval(model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gordon_df = pd.read_csv('onlyengandsentimenttotext.csv', encoding='utf8', dtype=str, escapechar='\\\\')\n",
    "\n",
    "\n",
    "def predict_sentence_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = word_tokenize(sentence)\n",
    "    indexed = [txt_field.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    output = model(tensor.to(device), length_tensor)\n",
    "    prediction = torch.round(torch.sigmoid(output)).int()\n",
    "    return prediction.cpu().numpy()[0]\n",
    "\n",
    "def predict_sentiment(row):\n",
    "    row[emotions_list] = predict_sentence_sentiment(model, row['text'])\n",
    "    return row\n",
    "\n",
    "for e in emotions_list:\n",
    "    gordon_df[e] = 0\n",
    "gordon_df = gordon_df.apply(predict_sentiment, axis=1)\n",
    "gordon_df.to_csv('onlyengandsentimenttotext_sentiment.csv', header=True, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-13 12:10:38</td>\n",
       "      <td>thagnome70065</td>\n",
       "      <td>Nice read.   Trump passes coronavirus test wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-13 12:10:39</td>\n",
       "      <td>drewthompson116</td>\n",
       "      <td>Stocks set to surge following worst day since ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-13 12:10:40</td>\n",
       "      <td>jorgenseptember</td>\n",
       "      <td>Maxine Waters Turns  Into A Political Crisis....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-13 12:10:40</td>\n",
       "      <td>padilloh</td>\n",
       "      <td>Katie Porter just saved countless lives. Than...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-13 12:10:40</td>\n",
       "      <td>_D_S_J_</td>\n",
       "      <td>Meanwhile in Egypt  they made someone dress u...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232755</th>\n",
       "      <td>2020-03-28 06:58:30</td>\n",
       "      <td>ViralDVora</td>\n",
       "      <td>Even a normal Sneezing and Cough a Normal Fev...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232756</th>\n",
       "      <td>2020-03-28 06:58:31</td>\n",
       "      <td>LicPravinw</td>\n",
       "      <td>In the news we heard that 80 lakh mobile users...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232757</th>\n",
       "      <td>2020-03-28 06:58:32</td>\n",
       "      <td>steffid06</td>\n",
       "      <td>As a diaspora member your communication matte...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232758</th>\n",
       "      <td>2020-03-28 06:58:32</td>\n",
       "      <td>ArcadiaT3</td>\n",
       "      <td>If u take a few letters from PANDEMIC you get ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232759</th>\n",
       "      <td>2020-03-28 06:58:36</td>\n",
       "      <td>AshabaEmmanuelJ</td>\n",
       "      <td>Very rare to find a young lady on Twitter ini...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1232760 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date             user  \\\n",
       "0        2020-03-13 12:10:38    thagnome70065   \n",
       "1        2020-03-13 12:10:39  drewthompson116   \n",
       "2        2020-03-13 12:10:40  jorgenseptember   \n",
       "3        2020-03-13 12:10:40         padilloh   \n",
       "4        2020-03-13 12:10:40          _D_S_J_   \n",
       "...                      ...              ...   \n",
       "1232755  2020-03-28 06:58:30       ViralDVora   \n",
       "1232756  2020-03-28 06:58:31       LicPravinw   \n",
       "1232757  2020-03-28 06:58:32        steffid06   \n",
       "1232758  2020-03-28 06:58:32        ArcadiaT3   \n",
       "1232759  2020-03-28 06:58:36  AshabaEmmanuelJ   \n",
       "\n",
       "                                                      text  anger  \\\n",
       "0         Nice read.   Trump passes coronavirus test wi...      0   \n",
       "1        Stocks set to surge following worst day since ...      0   \n",
       "2         Maxine Waters Turns  Into A Political Crisis....      0   \n",
       "3         Katie Porter just saved countless lives. Than...      0   \n",
       "4         Meanwhile in Egypt  they made someone dress u...      0   \n",
       "...                                                    ...    ...   \n",
       "1232755   Even a normal Sneezing and Cough a Normal Fev...      0   \n",
       "1232756  In the news we heard that 80 lakh mobile users...      0   \n",
       "1232757   As a diaspora member your communication matte...      0   \n",
       "1232758  If u take a few letters from PANDEMIC you get ...      0   \n",
       "1232759   Very rare to find a young lady on Twitter ini...      0   \n",
       "\n",
       "         anticipation  disgust  fear  joy  sadness  surprise  trust  neutral  \n",
       "0                   0        0     1    0        0         0      0        0  \n",
       "1                   0        0     1    0        1         1      1        0  \n",
       "2                   0        1     0    0        1         1      0        0  \n",
       "3                   0        0     0    0        0         0      0        1  \n",
       "4                   0        0     0    0        0         0      0        1  \n",
       "...               ...      ...   ...  ...      ...       ...    ...      ...  \n",
       "1232755             0        0     0    0        0         0      0        1  \n",
       "1232756             1        0     1    0        1         0      1        0  \n",
       "1232757             0        0     0    1        0         0      1        0  \n",
       "1232758             0        0     0    0        0         0      0        1  \n",
       "1232759             1        0     1    1        1         1      0        0  \n",
       "\n",
       "[1232760 rows x 12 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gordon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date            1232760\n",
       "user            1232760\n",
       "text            1232760\n",
       "anger            235652\n",
       "anticipation     305780\n",
       "disgust          162761\n",
       "fear             352080\n",
       "joy              247016\n",
       "sadness          274793\n",
       "surprise         159612\n",
       "trust            360131\n",
       "neutral          500139\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gordon_df.astype(bool).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>thagnome70065</td>\n",
       "      <td>Nice read.   Trump passes coronavirus test wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>drewthompson116</td>\n",
       "      <td>Stocks set to surge following worst day since ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>jorgenseptember</td>\n",
       "      <td>Maxine Waters Turns  Into A Political Crisis....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>padilloh</td>\n",
       "      <td>Katie Porter just saved countless lives. Than...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>_D_S_J_</td>\n",
       "      <td>Meanwhile in Egypt  they made someone dress u...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232755</th>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>ViralDVora</td>\n",
       "      <td>Even a normal Sneezing and Cough a Normal Fev...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232756</th>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>LicPravinw</td>\n",
       "      <td>In the news we heard that 80 lakh mobile users...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232757</th>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>steffid06</td>\n",
       "      <td>As a diaspora member your communication matte...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232758</th>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>ArcadiaT3</td>\n",
       "      <td>If u take a few letters from PANDEMIC you get ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232759</th>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>AshabaEmmanuelJ</td>\n",
       "      <td>Very rare to find a young lady on Twitter ini...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1232760 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date             user  \\\n",
       "0        2020-03-13    thagnome70065   \n",
       "1        2020-03-13  drewthompson116   \n",
       "2        2020-03-13  jorgenseptember   \n",
       "3        2020-03-13         padilloh   \n",
       "4        2020-03-13          _D_S_J_   \n",
       "...             ...              ...   \n",
       "1232755  2020-03-28       ViralDVora   \n",
       "1232756  2020-03-28       LicPravinw   \n",
       "1232757  2020-03-28        steffid06   \n",
       "1232758  2020-03-28        ArcadiaT3   \n",
       "1232759  2020-03-28  AshabaEmmanuelJ   \n",
       "\n",
       "                                                      text  anger  \\\n",
       "0         Nice read.   Trump passes coronavirus test wi...      0   \n",
       "1        Stocks set to surge following worst day since ...      0   \n",
       "2         Maxine Waters Turns  Into A Political Crisis....      0   \n",
       "3         Katie Porter just saved countless lives. Than...      0   \n",
       "4         Meanwhile in Egypt  they made someone dress u...      0   \n",
       "...                                                    ...    ...   \n",
       "1232755   Even a normal Sneezing and Cough a Normal Fev...      0   \n",
       "1232756  In the news we heard that 80 lakh mobile users...      0   \n",
       "1232757   As a diaspora member your communication matte...      0   \n",
       "1232758  If u take a few letters from PANDEMIC you get ...      0   \n",
       "1232759   Very rare to find a young lady on Twitter ini...      0   \n",
       "\n",
       "         anticipation  disgust  fear  joy  sadness  surprise  trust  neutral  \n",
       "0                   0        0     1    0        0         0      0        0  \n",
       "1                   0        0     1    0        1         1      1        0  \n",
       "2                   0        1     0    0        1         1      0        0  \n",
       "3                   0        0     0    0        0         0      0        1  \n",
       "4                   0        0     0    0        0         0      0        1  \n",
       "...               ...      ...   ...  ...      ...       ...    ...      ...  \n",
       "1232755             0        0     0    0        0         0      0        1  \n",
       "1232756             1        0     1    0        1         0      1        0  \n",
       "1232757             0        0     0    1        0         0      1        0  \n",
       "1232758             0        0     0    0        0         0      0        1  \n",
       "1232759             1        0     1    1        1         1      0        0  \n",
       "\n",
       "[1232760 rows x 12 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filterTime(row):\n",
    "    row['date'] = row['date'].split(\" \")[0]\n",
    "    return row\n",
    "\n",
    "gordon2_df = gordon_df.apply(filterTime, axis=1)\n",
    "gordon2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-03-13</th>\n",
       "      <td>9992</td>\n",
       "      <td>11916</td>\n",
       "      <td>7640</td>\n",
       "      <td>14874</td>\n",
       "      <td>9529</td>\n",
       "      <td>12332</td>\n",
       "      <td>6913</td>\n",
       "      <td>14240</td>\n",
       "      <td>20499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-14</th>\n",
       "      <td>11355</td>\n",
       "      <td>13887</td>\n",
       "      <td>8731</td>\n",
       "      <td>16817</td>\n",
       "      <td>11373</td>\n",
       "      <td>13892</td>\n",
       "      <td>7747</td>\n",
       "      <td>16151</td>\n",
       "      <td>23305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-15</th>\n",
       "      <td>14891</td>\n",
       "      <td>18755</td>\n",
       "      <td>11037</td>\n",
       "      <td>23054</td>\n",
       "      <td>14876</td>\n",
       "      <td>18595</td>\n",
       "      <td>9785</td>\n",
       "      <td>21757</td>\n",
       "      <td>32102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-16</th>\n",
       "      <td>16612</td>\n",
       "      <td>22422</td>\n",
       "      <td>11843</td>\n",
       "      <td>25685</td>\n",
       "      <td>18098</td>\n",
       "      <td>19918</td>\n",
       "      <td>11450</td>\n",
       "      <td>26385</td>\n",
       "      <td>37570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-17</th>\n",
       "      <td>11277</td>\n",
       "      <td>15316</td>\n",
       "      <td>8079</td>\n",
       "      <td>16990</td>\n",
       "      <td>12359</td>\n",
       "      <td>13252</td>\n",
       "      <td>7743</td>\n",
       "      <td>17793</td>\n",
       "      <td>24796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-18</th>\n",
       "      <td>18587</td>\n",
       "      <td>25495</td>\n",
       "      <td>12948</td>\n",
       "      <td>28922</td>\n",
       "      <td>20250</td>\n",
       "      <td>22154</td>\n",
       "      <td>13250</td>\n",
       "      <td>29719</td>\n",
       "      <td>39431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-19</th>\n",
       "      <td>18156</td>\n",
       "      <td>23174</td>\n",
       "      <td>12388</td>\n",
       "      <td>26491</td>\n",
       "      <td>18698</td>\n",
       "      <td>20438</td>\n",
       "      <td>11867</td>\n",
       "      <td>27151</td>\n",
       "      <td>37206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-20</th>\n",
       "      <td>18927</td>\n",
       "      <td>25313</td>\n",
       "      <td>13233</td>\n",
       "      <td>27947</td>\n",
       "      <td>20466</td>\n",
       "      <td>21748</td>\n",
       "      <td>12866</td>\n",
       "      <td>29756</td>\n",
       "      <td>40225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-21</th>\n",
       "      <td>17109</td>\n",
       "      <td>22358</td>\n",
       "      <td>11690</td>\n",
       "      <td>25898</td>\n",
       "      <td>18340</td>\n",
       "      <td>19774</td>\n",
       "      <td>11668</td>\n",
       "      <td>26624</td>\n",
       "      <td>37513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-22</th>\n",
       "      <td>18550</td>\n",
       "      <td>23255</td>\n",
       "      <td>12664</td>\n",
       "      <td>27062</td>\n",
       "      <td>18776</td>\n",
       "      <td>20888</td>\n",
       "      <td>12074</td>\n",
       "      <td>26968</td>\n",
       "      <td>42151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-23</th>\n",
       "      <td>8591</td>\n",
       "      <td>10253</td>\n",
       "      <td>5717</td>\n",
       "      <td>12334</td>\n",
       "      <td>8452</td>\n",
       "      <td>9482</td>\n",
       "      <td>5350</td>\n",
       "      <td>12384</td>\n",
       "      <td>17597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-24</th>\n",
       "      <td>17958</td>\n",
       "      <td>24362</td>\n",
       "      <td>11830</td>\n",
       "      <td>27492</td>\n",
       "      <td>19922</td>\n",
       "      <td>20882</td>\n",
       "      <td>12657</td>\n",
       "      <td>29251</td>\n",
       "      <td>36216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-25</th>\n",
       "      <td>18242</td>\n",
       "      <td>25112</td>\n",
       "      <td>11998</td>\n",
       "      <td>27196</td>\n",
       "      <td>20257</td>\n",
       "      <td>21021</td>\n",
       "      <td>13294</td>\n",
       "      <td>29361</td>\n",
       "      <td>40562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-26</th>\n",
       "      <td>13855</td>\n",
       "      <td>18147</td>\n",
       "      <td>8823</td>\n",
       "      <td>20656</td>\n",
       "      <td>14655</td>\n",
       "      <td>16202</td>\n",
       "      <td>9395</td>\n",
       "      <td>21587</td>\n",
       "      <td>27967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-27</th>\n",
       "      <td>15216</td>\n",
       "      <td>18605</td>\n",
       "      <td>9746</td>\n",
       "      <td>21806</td>\n",
       "      <td>15057</td>\n",
       "      <td>17147</td>\n",
       "      <td>9549</td>\n",
       "      <td>22255</td>\n",
       "      <td>30716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-28</th>\n",
       "      <td>6334</td>\n",
       "      <td>7410</td>\n",
       "      <td>4394</td>\n",
       "      <td>8856</td>\n",
       "      <td>5908</td>\n",
       "      <td>7068</td>\n",
       "      <td>4004</td>\n",
       "      <td>8749</td>\n",
       "      <td>12283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            anger  anticipation  disgust   fear    joy  sadness  surprise  \\\n",
       "date                                                                        \n",
       "2020-03-13   9992         11916     7640  14874   9529    12332      6913   \n",
       "2020-03-14  11355         13887     8731  16817  11373    13892      7747   \n",
       "2020-03-15  14891         18755    11037  23054  14876    18595      9785   \n",
       "2020-03-16  16612         22422    11843  25685  18098    19918     11450   \n",
       "2020-03-17  11277         15316     8079  16990  12359    13252      7743   \n",
       "2020-03-18  18587         25495    12948  28922  20250    22154     13250   \n",
       "2020-03-19  18156         23174    12388  26491  18698    20438     11867   \n",
       "2020-03-20  18927         25313    13233  27947  20466    21748     12866   \n",
       "2020-03-21  17109         22358    11690  25898  18340    19774     11668   \n",
       "2020-03-22  18550         23255    12664  27062  18776    20888     12074   \n",
       "2020-03-23   8591         10253     5717  12334   8452     9482      5350   \n",
       "2020-03-24  17958         24362    11830  27492  19922    20882     12657   \n",
       "2020-03-25  18242         25112    11998  27196  20257    21021     13294   \n",
       "2020-03-26  13855         18147     8823  20656  14655    16202      9395   \n",
       "2020-03-27  15216         18605     9746  21806  15057    17147      9549   \n",
       "2020-03-28   6334          7410     4394   8856   5908     7068      4004   \n",
       "\n",
       "            trust  neutral  \n",
       "date                        \n",
       "2020-03-13  14240    20499  \n",
       "2020-03-14  16151    23305  \n",
       "2020-03-15  21757    32102  \n",
       "2020-03-16  26385    37570  \n",
       "2020-03-17  17793    24796  \n",
       "2020-03-18  29719    39431  \n",
       "2020-03-19  27151    37206  \n",
       "2020-03-20  29756    40225  \n",
       "2020-03-21  26624    37513  \n",
       "2020-03-22  26968    42151  \n",
       "2020-03-23  12384    17597  \n",
       "2020-03-24  29251    36216  \n",
       "2020-03-25  29361    40562  \n",
       "2020-03-26  21587    27967  \n",
       "2020-03-27  22255    30716  \n",
       "2020-03-28   8749    12283  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = ['date','anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust', 'neutral']\n",
    "gordon3_df = gordon2_df[arr].groupby('date').sum()\n",
    "gordon3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gordon3_df.to_csv('onlyengandsentimenttotext_sentiment_date.csv', header=True, index=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-03-13</th>\n",
       "      <td>0.092574</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>0.070783</td>\n",
       "      <td>0.137805</td>\n",
       "      <td>0.088285</td>\n",
       "      <td>0.114254</td>\n",
       "      <td>0.064048</td>\n",
       "      <td>0.131931</td>\n",
       "      <td>0.189920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-14</th>\n",
       "      <td>0.092124</td>\n",
       "      <td>0.112666</td>\n",
       "      <td>0.070835</td>\n",
       "      <td>0.136437</td>\n",
       "      <td>0.092270</td>\n",
       "      <td>0.112707</td>\n",
       "      <td>0.062852</td>\n",
       "      <td>0.131034</td>\n",
       "      <td>0.189075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-15</th>\n",
       "      <td>0.090330</td>\n",
       "      <td>0.113769</td>\n",
       "      <td>0.066951</td>\n",
       "      <td>0.139847</td>\n",
       "      <td>0.090239</td>\n",
       "      <td>0.112798</td>\n",
       "      <td>0.059356</td>\n",
       "      <td>0.131979</td>\n",
       "      <td>0.194732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-16</th>\n",
       "      <td>0.087439</td>\n",
       "      <td>0.118021</td>\n",
       "      <td>0.062337</td>\n",
       "      <td>0.135196</td>\n",
       "      <td>0.095261</td>\n",
       "      <td>0.104841</td>\n",
       "      <td>0.060269</td>\n",
       "      <td>0.138881</td>\n",
       "      <td>0.197755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-17</th>\n",
       "      <td>0.088374</td>\n",
       "      <td>0.120027</td>\n",
       "      <td>0.063313</td>\n",
       "      <td>0.133145</td>\n",
       "      <td>0.096854</td>\n",
       "      <td>0.103852</td>\n",
       "      <td>0.060679</td>\n",
       "      <td>0.139438</td>\n",
       "      <td>0.194318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-18</th>\n",
       "      <td>0.088192</td>\n",
       "      <td>0.120969</td>\n",
       "      <td>0.061436</td>\n",
       "      <td>0.137230</td>\n",
       "      <td>0.096083</td>\n",
       "      <td>0.105117</td>\n",
       "      <td>0.062869</td>\n",
       "      <td>0.141011</td>\n",
       "      <td>0.187093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-19</th>\n",
       "      <td>0.092837</td>\n",
       "      <td>0.118495</td>\n",
       "      <td>0.063343</td>\n",
       "      <td>0.135456</td>\n",
       "      <td>0.095608</td>\n",
       "      <td>0.104505</td>\n",
       "      <td>0.060679</td>\n",
       "      <td>0.138831</td>\n",
       "      <td>0.190245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-20</th>\n",
       "      <td>0.089923</td>\n",
       "      <td>0.120263</td>\n",
       "      <td>0.062870</td>\n",
       "      <td>0.132777</td>\n",
       "      <td>0.097234</td>\n",
       "      <td>0.103325</td>\n",
       "      <td>0.061127</td>\n",
       "      <td>0.141371</td>\n",
       "      <td>0.191110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-21</th>\n",
       "      <td>0.089588</td>\n",
       "      <td>0.117074</td>\n",
       "      <td>0.061213</td>\n",
       "      <td>0.135610</td>\n",
       "      <td>0.096034</td>\n",
       "      <td>0.103543</td>\n",
       "      <td>0.061097</td>\n",
       "      <td>0.139412</td>\n",
       "      <td>0.196430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-22</th>\n",
       "      <td>0.091656</td>\n",
       "      <td>0.114903</td>\n",
       "      <td>0.062573</td>\n",
       "      <td>0.133713</td>\n",
       "      <td>0.092772</td>\n",
       "      <td>0.103208</td>\n",
       "      <td>0.059658</td>\n",
       "      <td>0.133249</td>\n",
       "      <td>0.208268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-23</th>\n",
       "      <td>0.095286</td>\n",
       "      <td>0.113720</td>\n",
       "      <td>0.063409</td>\n",
       "      <td>0.136801</td>\n",
       "      <td>0.093744</td>\n",
       "      <td>0.105169</td>\n",
       "      <td>0.059339</td>\n",
       "      <td>0.137356</td>\n",
       "      <td>0.195175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-24</th>\n",
       "      <td>0.089535</td>\n",
       "      <td>0.121464</td>\n",
       "      <td>0.058982</td>\n",
       "      <td>0.137069</td>\n",
       "      <td>0.099327</td>\n",
       "      <td>0.104113</td>\n",
       "      <td>0.063105</td>\n",
       "      <td>0.145839</td>\n",
       "      <td>0.180565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-25</th>\n",
       "      <td>0.088107</td>\n",
       "      <td>0.121289</td>\n",
       "      <td>0.057949</td>\n",
       "      <td>0.131354</td>\n",
       "      <td>0.097840</td>\n",
       "      <td>0.101530</td>\n",
       "      <td>0.064209</td>\n",
       "      <td>0.141811</td>\n",
       "      <td>0.195911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-26</th>\n",
       "      <td>0.091581</td>\n",
       "      <td>0.119951</td>\n",
       "      <td>0.058320</td>\n",
       "      <td>0.136535</td>\n",
       "      <td>0.096869</td>\n",
       "      <td>0.107094</td>\n",
       "      <td>0.062101</td>\n",
       "      <td>0.142689</td>\n",
       "      <td>0.184861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-27</th>\n",
       "      <td>0.095042</td>\n",
       "      <td>0.116211</td>\n",
       "      <td>0.060876</td>\n",
       "      <td>0.136205</td>\n",
       "      <td>0.094049</td>\n",
       "      <td>0.107104</td>\n",
       "      <td>0.059645</td>\n",
       "      <td>0.139009</td>\n",
       "      <td>0.191859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-28</th>\n",
       "      <td>0.097437</td>\n",
       "      <td>0.113989</td>\n",
       "      <td>0.067594</td>\n",
       "      <td>0.136234</td>\n",
       "      <td>0.090884</td>\n",
       "      <td>0.108728</td>\n",
       "      <td>0.061594</td>\n",
       "      <td>0.134588</td>\n",
       "      <td>0.188952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               anger  anticipation   disgust      fear       joy   sadness  \\\n",
       "date                                                                         \n",
       "2020-03-13  0.092574      0.110400  0.070783  0.137805  0.088285  0.114254   \n",
       "2020-03-14  0.092124      0.112666  0.070835  0.136437  0.092270  0.112707   \n",
       "2020-03-15  0.090330      0.113769  0.066951  0.139847  0.090239  0.112798   \n",
       "2020-03-16  0.087439      0.118021  0.062337  0.135196  0.095261  0.104841   \n",
       "2020-03-17  0.088374      0.120027  0.063313  0.133145  0.096854  0.103852   \n",
       "2020-03-18  0.088192      0.120969  0.061436  0.137230  0.096083  0.105117   \n",
       "2020-03-19  0.092837      0.118495  0.063343  0.135456  0.095608  0.104505   \n",
       "2020-03-20  0.089923      0.120263  0.062870  0.132777  0.097234  0.103325   \n",
       "2020-03-21  0.089588      0.117074  0.061213  0.135610  0.096034  0.103543   \n",
       "2020-03-22  0.091656      0.114903  0.062573  0.133713  0.092772  0.103208   \n",
       "2020-03-23  0.095286      0.113720  0.063409  0.136801  0.093744  0.105169   \n",
       "2020-03-24  0.089535      0.121464  0.058982  0.137069  0.099327  0.104113   \n",
       "2020-03-25  0.088107      0.121289  0.057949  0.131354  0.097840  0.101530   \n",
       "2020-03-26  0.091581      0.119951  0.058320  0.136535  0.096869  0.107094   \n",
       "2020-03-27  0.095042      0.116211  0.060876  0.136205  0.094049  0.107104   \n",
       "2020-03-28  0.097437      0.113989  0.067594  0.136234  0.090884  0.108728   \n",
       "\n",
       "            surprise     trust   neutral  \n",
       "date                                      \n",
       "2020-03-13  0.064048  0.131931  0.189920  \n",
       "2020-03-14  0.062852  0.131034  0.189075  \n",
       "2020-03-15  0.059356  0.131979  0.194732  \n",
       "2020-03-16  0.060269  0.138881  0.197755  \n",
       "2020-03-17  0.060679  0.139438  0.194318  \n",
       "2020-03-18  0.062869  0.141011  0.187093  \n",
       "2020-03-19  0.060679  0.138831  0.190245  \n",
       "2020-03-20  0.061127  0.141371  0.191110  \n",
       "2020-03-21  0.061097  0.139412  0.196430  \n",
       "2020-03-22  0.059658  0.133249  0.208268  \n",
       "2020-03-23  0.059339  0.137356  0.195175  \n",
       "2020-03-24  0.063105  0.145839  0.180565  \n",
       "2020-03-25  0.064209  0.141811  0.195911  \n",
       "2020-03-26  0.062101  0.142689  0.184861  \n",
       "2020-03-27  0.059645  0.139009  0.191859  \n",
       "2020-03-28  0.061594  0.134588  0.188952  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gordon4_df = gordon3_df.divide(gordon3_df.sum(axis=1),axis=0)\n",
    "gordon4_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter RT@user:\n",
    "# filter @user\n",
    "# filter #hashtag\n",
    "# filter emoji\n",
    "# filter non-eng\n",
    "# pass data to model\n",
    "# change format of time\n",
    "# calculate percentage of each emotion\n",
    "# calculate relationship with new infected count\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNGx9WW04mA1MyM/L5U+od0",
   "collapsed_sections": [],
   "name": "20505654_code2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
